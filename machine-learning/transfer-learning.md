# 迁移学习
### 迁移学习
#### 迁移学习的必要性

| 矛盾 | 传统机器学习 | 迁移学习 |
| ------ | ------ | ------ |
|大数据与少标注|增加人工标注，但昂贵且耗时 | 标注迁移 |
|大数据与弱计算|只依赖强大的计算能力，但受众少|模型迁移| 
|普适化模型与个性化需求|通用模型无法满足个性化需求|模型自适应调整|
|特定应用|冷启动问题无法解决|数据迁移|
####与已有概率的区别和联系
1. 迁移学习 vs 传统机器学习

|比较项目|传统机器学习|迁移学习|
|------|------|------|
|数据分布|训练集和测试集同分布|训练集和测试集服从不同的分布|
|数据标注|需要足够的数据标注来训练模型|不需要足够的标注数据|
|模型|每个任务分别建模|模型可以在不同任务之间迁移|

2. 迁移学习 vs 多任务学习  

   多任务学习指多个相关的任务一起协同学习， 迁移学习强调知识由一个领域迁移到另一个领域的过程。迁移是思想，多任务是其中的一种具体形式。
   
3. 迁移学习 vs 终身学习

   终身学习可以认为是序列化的多任务学习，在已经学习好若干个任务后，面对新的任务可以继续学习而不会遗忘之前学习的任务。迁移学习则侧重于模型的迁移和共同学习
   
4. 迁移学习 vs 领域自适应
   领域自适应问题是迁移学习的研究内容之一，它侧重于解决特征空间一致，类别空间一致，仅特征分布不一致的问题。而迁移学习可以解决上述内容的不一致问题
   
5. 迁移学习 vs 增量学习
   增量学习侧重解决数据不断到来，模型不断更新的问题，迁移学习显然和其有着不同之处
   
6. 迁移学习 vs 自我学习 
   自我学习指的是模型不断从自身处进行更新，而迁移学习强调知识在不同领域之间迁移
   
7. 迁移学习 vs 协方差漂移
   协方差漂移指的是数据的边缘概率分布发生变化，领域自适应问题研究解决此类问题
   
#### 负迁移
   在源领域学习的知识，对于目标域上的学习产生负面作用。主要原因：
   - 数据问题： 源域和目标域压根不相似，谈何迁移
   - 方法问题： 源域和目标域是相似的， 但迁移方法不够好， 没有找到可迁移的成分
   
#### 最新研究成果
- 传递迁移学习
- 远领域迁移学习

#### 迁移学习的研究领域

1. 按目标域的标签分
    - 监督迁移学习
    - 半监督迁移学习
    - 无监督迁移学习
2. 按学习方法分类
    - 基于的样本的迁移学习
    - 基于特征的迁移学习
    - 基于模型的迁移学习
    - 基于关系的迁移学习
3. 按特征分类
    - 同构迁移学习 homogeneous transfer learning
    - 异构迁移学习 heterogeneous transfer learning
4. 按在线、离线分
    - 离线迁移学习
    - 在线迁移学习
    
#### 应用领域
- 计算机视觉
- 文本分类
- 时间序列
- 医疗健康

#### 迁移学习的度量准则


#### 迁移学习的基本方法
1. 基于样本的迁移
   根据一定的权重生成规则，对数据样本进行重用，来进行迁移学习。一般假设：
   - $$P(x_s) \neq P(x_t)$$
   - 直接估计 $$P(x_s)$$或$$ P(x_t)$$不可行
   - 一般假设$$\frac{P(x_s)} {P(x_t)} \lt \infty $$, 进行求$$\frac{P(x_s)} {P(x_t)} $$
2. 基于特征的迁移
   通过特征变换的方法互相迁移， 来减少源域或目标域的差距。或者将源域和目标域的数据特征变换到同一特征空间。

3. 基于模型的迁移
   从源域和目标域中找到他们之间共享的参数信息。假设：
   - 源域和目标域可以共享一些模型的参数
   
4. 基于关系的迁移
   关于源域和目标域的样本关系。 一般基于马尔科夫逻辑网络来挖掘不同领域之间的关系。

#### 数据分布自适应
   基本思想： 由于源域和目标域的数据概率分布不同，那么最直接的方式是通过一些变换，将不同的数据分布的距离拉近。
1. 边缘分布自适应
   - 目标： 减小源域和目标域的边缘概率分布的距离，从而完成迁移学习。 $$DISTANCE(D_s, D_t) \approx ||P(x_s) - P(x_t)|| $$
   - 迁移成分分析： 直接减少二者的距离是不行的， 假设存在一个特征映射$$\phi$$, 映射后数据的分布：$$P(\phi(x_s)) \approx P(\phi(x_t))$$, 找到合适的$$\phi$$即可
   - MMD： maximum mean discrepancy 假设$$\phi$$已知， 求距离： $$DISTANCE(x_s, x_t) \approx ||\frac{1}{n_1}\sum_{i=1}^{n_1}\phi(x_i) - \frac{1}{n_2}\sum_{j=1}^{n_2}\phi(x_j) ||$$, 即映射后的均值之差。 
   - 把MMD平方后，有乘积项，联想SVM中的核函数，　把难求的距离变成了$$tr(KL) - \lambda tr(K)$$ tr:矩阵的迹
   - 由于该问题一个semi-definite programming. 解决起来耗时，故考虑降维。
   
2. 条件分布自适应
   - 目标： 减小源域与目标域条件概率分布的距离， 来实现迁移学习。

3. 联合分布自适应
   - 目标： 减小源域与目标域的联合概率分布的距离。

#### 特征选择
   基本假设： 源域和目标域均含有一部分的公共特征，在这部分公共的特征上，源域和目标域的数据分布是一致的。因此，此类方法的目的就是找到这部分共享的特征，完成迁移学习。
   
#### 子空间学习
   基本假设: 源域和目标域在变换后的子空间中会有着相似的分布。主要分两种： 基于统计特征变换的统计特征对齐方法和基于流形变换的流形学习方法。
1. 统计特征对齐
   - 主要将数据的统计特征进行变换对齐。对齐后的数据， 可以利用传统机器学习方法构建分类器进行学习。
2. 流形学习
   - 假设： 现有数据是从一个高维空间中采样出来的， 所有它具有高维空间中的低维流形结构。

#### 深度迁移学习




